{
  "cells": [
    {
      "metadata": {
        "id": "92TaXYOcSR95"
      },
      "cell_type": "markdown",
      "source": [
        "# Read_and_explore_data"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "6diTTs6OSR95"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "# Libraries and packages for text (pre-)processing\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"Version info.:\", sys.version_info)\n",
        "print(\"pandas version:\", pd.__version__)\n",
        "print(\"numpy version:\", np.__version__)\n",
        "print(\"skearn version:\", sklearn.__version__)\n",
        "print(\"re version:\", re.__version__)\n",
        "print(\"nltk version:\", nltk.__version__)\n",
        "\n",
        "for dirname, _, filenames in os.walk('/Data/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7jG7Q399SR95"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"Read_the_Data\"></a>\n",
        "## Read the Data"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lqc1OWW9SR96"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "# read the csv file\n",
        "train_df = pd.read_csv(\"/Data/train.csv\")\n",
        "display(train_df.shape, train_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "xKtuFjCZSR96"
      },
      "cell_type": "code",
      "source": [
        "display(train_df[~train_df[\"location\"].isnull()].head())\n",
        "display(train_df[train_df[\"target\"] == 0][\"text\"].values[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kkZK38tmSR96"
      },
      "cell_type": "markdown",
      "source": [
        "# Text_Cleaning"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "9qOYwLD6SR97"
      },
      "cell_type": "code",
      "source": [
        "train_df[\"text_clean\"] = train_df[\"text\"].apply(lambda x: x.lower())\n",
        "display(train_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "boReNKtRSR98"
      },
      "cell_type": "code",
      "source": [
        "def remove_URL(text):\n",
        "    \"\"\"\n",
        "        Remove URLs from a sample string\n",
        "    \"\"\"\n",
        "    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "KG2pndeqSR98"
      },
      "cell_type": "code",
      "source": [
        "# remove urls from the text\n",
        "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_URL(x))\n",
        "\n",
        "# double check\n",
        "print(train_df[\"text\"][31])\n",
        "print(train_df[\"text_clean\"][31])\n",
        "print(train_df[\"text\"][37])\n",
        "print(train_df[\"text_clean\"][37])\n",
        "print(train_df[\"text\"][62])\n",
        "print(train_df[\"text_clean\"][62])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "NVfoqGljSR98"
      },
      "cell_type": "code",
      "source": [
        "def remove_html(text):\n",
        "    \"\"\"\n",
        "        Remove the html in sample text\n",
        "    \"\"\"\n",
        "    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
        "    return re.sub(html, \"\", text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "Ta5XGxX3SR99"
      },
      "cell_type": "code",
      "source": [
        "# remove html from the text\n",
        "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_html(x))\n",
        "\n",
        "# double check\n",
        "print(train_df[\"text\"][62])\n",
        "print(train_df[\"text_clean\"][62])\n",
        "print(train_df[\"text\"][7385])\n",
        "print(train_df[\"text_clean\"][7385])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "6b2RyqljSR99"
      },
      "cell_type": "code",
      "source": [
        "def remove_non_ascii(text):\n",
        "    \"\"\"\n",
        "        Remove non-ASCII characters\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^\\x00-\\x7f]',r'', text) # or ''.join([x for x in text if x in string.printable])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "i4pca4tkSR99"
      },
      "cell_type": "code",
      "source": [
        "# remove non-ascii characters from the text\n",
        "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_non_ascii(x))\n",
        "\n",
        "# double check\n",
        "print(train_df[\"text\"][38])\n",
        "print(train_df[\"text_clean\"][38])\n",
        "print(train_df[\"text\"][7586])\n",
        "print(train_df[\"text_clean\"][7586])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "ZNfJRY_OSR9-"
      },
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text):\n",
        "    \"\"\"\n",
        "        Remove special special characters, including symbols, emojis, and other graphic characters\n",
        "    \"\"\"\n",
        "    emoji_pattern = re.compile(\n",
        "        '['\n",
        "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
        "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
        "        u'\\U00002702-\\U000027B0'\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+',\n",
        "        flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9OalcE_kSR9-"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "# remove non-ascii characters from the text\n",
        "train_df_jtcc[\"text_clean\"] = train_df_jtcc[\"comment_text\"].apply(lambda x: remove_special_characters(x))\n",
        "display(train_df_jtcc.head())\n",
        "\n",
        "# double check\n",
        "print(train_df_jtcc[\"comment_text\"][143])\n",
        "print(train_df_jtcc[\"text_clean\"][143])\n",
        "print(train_df_jtcc[\"comment_text\"][189])\n",
        "print(train_df_jtcc[\"text_clean\"][189])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "B2pA2LvySR9-"
      },
      "cell_type": "code",
      "source": [
        "def remove_punct(text):\n",
        "    \"\"\"\n",
        "        Remove the punctuation\n",
        "    \"\"\"\n",
        "#     return re.sub(r'[]!\"$%&\\'()*+,./:;=#@?[\\\\^_`{|}~-]+', \"\", text)\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "lnj_ZVvYSR-A"
      },
      "cell_type": "code",
      "source": [
        "# remove punctuations from the text\n",
        "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_punct(x))\n",
        "\n",
        "# double check\n",
        "print(train_df[\"text\"][5])\n",
        "print(train_df[\"text_clean\"][5])\n",
        "print(train_df[\"text\"][7597])\n",
        "print(train_df[\"text_clean\"][7597])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ks7-oCXxSR-C"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing:\n",
        "\n",
        "## Tokenization\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "ju3SBXyJSR-C"
      },
      "cell_type": "code",
      "source": [
        "# Tokenizing the tweet base texts.\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "train_df['tokenized'] = train_df['text_clean'].apply(word_tokenize)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "6rb89qOySR-H"
      },
      "cell_type": "code",
      "source": [
        "# Removing stopwords.\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "train_df['stopwords_removed'] = train_df['tokenized'].apply(lambda x: [word for word in x if word not in stop])\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "1HcDpKgASR-I"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def porter_stemmer(text):\n",
        "    \"\"\"\n",
        "        Stem words in list of tokenized words with PorterStemmer\n",
        "    \"\"\"\n",
        "    stemmer = nltk.PorterStemmer()\n",
        "    stems = [stemmer.stem(i) for i in text]\n",
        "    return stems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "IkUoV71oSR-I"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "train_df['porter_stemmer'] = train_df['stopwords_removed'].apply(lambda x: porter_stemmer(x))\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "J4mPzDfCSR-I"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "def snowball_stemmer(text):\n",
        "    \"\"\"\n",
        "        Stem words in list of tokenized words with SnowballStemmer\n",
        "    \"\"\"\n",
        "    stemmer = nltk.SnowballStemmer(\"english\")\n",
        "    stems = [stemmer.stem(i) for i in text]\n",
        "    return stems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "nRmuUZQwSR-I"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "train_df['snowball_stemmer'] = train_df['stopwords_removed'].apply(lambda x: snowball_stemmer(x))\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "Vw4lV8vbSR-J"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "def lancaster_stemmer(text):\n",
        "    \"\"\"\n",
        "        Stem words in list of tokenized words with LancasterStemmer\n",
        "    \"\"\"\n",
        "    stemmer = nltk.LancasterStemmer()\n",
        "    stems = [stemmer.stem(i) for i in text]\n",
        "    return stems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0ZEZDY6oSR-J"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "train_df['lancaster_stemmer'] = train_df['stopwords_removed'].apply(lambda x: lancaster_stemmer(x))\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FhinALeISR-J"
      },
      "cell_type": "markdown",
      "source": [
        "## Part of Speech Tagging (POS Tagging):\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "ZC91OOZySR-J"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import brown\n",
        "\n",
        "wordnet_map = {\"N\":wordnet.NOUN,\n",
        "               \"V\":wordnet.VERB,\n",
        "               \"J\":wordnet.ADJ,\n",
        "               \"R\":wordnet.ADV\n",
        "              }\n",
        "\n",
        "train_sents = brown.tagged_sents(categories='news')\n",
        "t0 = nltk.DefaultTagger('NN')\n",
        "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
        "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
        "\n",
        "def pos_tag_wordnet(text, pos_tag_type=\"pos_tag\"):\n",
        "    \"\"\"\n",
        "        Create pos_tag with wordnet format\n",
        "    \"\"\"\n",
        "    pos_tagged_text = t2.tag(text)\n",
        "\n",
        "    # map the pos tagging output with wordnet output\n",
        "    pos_tagged_text = [(word, wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys() else (word, wordnet.NOUN) for (word, pos_tag) in pos_tagged_text ]\n",
        "    return pos_tagged_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "GXGC5tdMSR-J"
      },
      "cell_type": "code",
      "source": [
        "pos_tag_wordnet(train_df['stopwords_removed'][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0JcnHYupSR-K"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "train_df['combined_postag_wnet'] = train_df['stopwords_removed'].apply(lambda x: pos_tag_wordnet(x))\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f1_I2CLZSR-K"
      },
      "cell_type": "markdown",
      "source": [
        "## Lemmatization:\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "yf93ZRObSR-K"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatize_word(text):\n",
        "    \"\"\"\n",
        "        Lemmatize the tokenized words\n",
        "    \"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]\n",
        "    return lemma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3-EkKDpBSR-L"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "# Test with POS Tagging\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "train_df['lemmatize_word_w_pos'] = train_df['combined_postag_wnet'].apply(lambda x: lemmatize_word(x))\n",
        "train_df['lemmatize_word_w_pos'] = train_df['lemmatize_word_w_pos'].apply(lambda x: [word for word in x if word not in stop]) # double check to remove stop words\n",
        "train_df['lemmatize_text'] = [' '.join(map(str, l)) for l in train_df['lemmatize_word_w_pos']] # join back to text\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w8Oa2kSmSR-N"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Features Extraction:\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "HZIErcMnSR-N"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def cv(data, ngram = 1, MAX_NB_WORDS = 75000):\n",
        "    count_vectorizer = CountVectorizer(ngram_range = (ngram, ngram), max_features = MAX_NB_WORDS)\n",
        "    emb = count_vectorizer.fit_transform(data).toarray()\n",
        "    print(\"count vectorize with\", str(np.array(emb).shape[1]), \"features\")\n",
        "    return emb, count_vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "LA7ULEX5SR-N"
      },
      "cell_type": "code",
      "source": [
        "def print_out(emb, feat, ngram, compared_sentence=0):\n",
        "    print(ngram,\"bag-of-words: \")\n",
        "    print(feat.get_feature_names(), \"\\n\")\n",
        "    print(ngram,\"bag-of-feature: \")\n",
        "    print(test_cv_1gram.vocabulary_, \"\\n\")\n",
        "    print(\"BoW matrix:\")\n",
        "    print(pd.DataFrame(emb.transpose(), index = feat.get_feature_names()).head(), \"\\n\")\n",
        "    print(ngram,\"vector example:\")\n",
        "    print(train_df[\"lemmatize_text\"][compared_sentence])\n",
        "    print(emb[compared_sentence], \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "JZVNQgZhSR-O"
      },
      "cell_type": "code",
      "source": [
        "test_corpus = train_df[\"lemmatize_text\"][:5].tolist()\n",
        "print(\"The test corpus: \", test_corpus, \"\\n\")\n",
        "\n",
        "test_cv_em_1gram, test_cv_1gram = cv(test_corpus, ngram=1)\n",
        "print_out(test_cv_em_1gram, test_cv_1gram, ngram=\"Uni-gram\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "UpuQPX7ISR-O"
      },
      "cell_type": "code",
      "source": [
        "test_cv_em_2gram, test_cv_2gram = cv(test_corpus, ngram=2)\n",
        "print_out(test_cv_em_2gram, test_cv_2gram, ngram=\"Bi-gram\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PBgWMko3SR-O"
      },
      "cell_type": "code",
      "source": [
        "test_cv_em_3gram, test_cv_3gram = cv(test_corpus, ngram=3)\n",
        "print_out(test_cv_em_2gram, test_cv_2gram, ngram=\"Tri-gram\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "EBnK4N7ySR-O"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "# implement into the whole dataset\n",
        "train_df_corpus = train_df[\"lemmatize_text\"].tolist()\n",
        "train_df_em_1gram, vc_1gram = cv(train_df_corpus, 1)\n",
        "train_df_em_2gram, vc_2gram = cv(train_df_corpus, 2)\n",
        "train_df_em_3gram, vc_3gram = cv(train_df_corpus, 3)\n",
        "\n",
        "print(len(train_df_corpus))\n",
        "print(train_df_em_1gram.shape)\n",
        "print(train_df_em_2gram.shape)\n",
        "print(train_df_em_3gram.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "44oF1ZIpSR-O"
      },
      "cell_type": "code",
      "source": [
        "del train_df_em_1gram, train_df_em_2gram, train_df_em_3gram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NqAVoF0jSR-P"
      },
      "cell_type": "markdown",
      "source": [
        "### Term Frequency-Inverse Document Frequency (TF-IDF):\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "YCveInXGSR-P"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def TFIDF(data, ngram = 1, MAX_NB_WORDS = 75000):\n",
        "    tfidf_x = TfidfVectorizer(ngram_range = (ngram, ngram), max_features = MAX_NB_WORDS)\n",
        "    emb = tfidf_x.fit_transform(data).toarray()\n",
        "    print(\"tf-idf with\", str(np.array(emb).shape[1]), \"features\")\n",
        "    return emb, tfidf_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "q3ZCRE_wSR-P"
      },
      "cell_type": "code",
      "source": [
        "test_corpus = train_df[\"lemmatize_text\"][:5].tolist()\n",
        "print(\"The test corpus: \", test_corpus, \"\\n\")\n",
        "\n",
        "test_tfidf_em_1gram, test_tfidf_1gram = TFIDF(test_corpus, ngram=1)\n",
        "print_out(test_tfidf_em_1gram, test_tfidf_1gram, ngram=\"Uni-gram\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2fMjFU7lSR-P"
      },
      "cell_type": "code",
      "source": [
        "test_tfidf_em_2gram, test_tfidf_2gram = TFIDF(test_corpus, ngram=2)\n",
        "print_out(test_tfidf_em_2gram, test_tfidf_2gram, ngram=\"Bi-gram\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fmRIVXa0SR-P"
      },
      "cell_type": "code",
      "source": [
        "test_tfidf_em_3gram, test_tfidf_3gram = TFIDF(test_corpus, ngram=3)\n",
        "print_out(test_tfidf_em_3gram, test_tfidf_3gram, ngram=\"Tri-gram\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "EcBEKy97SR-P"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "# implement into the whole dataset\n",
        "train_df_corpus = train_df[\"lemmatize_text\"].tolist()\n",
        "train_df_tfidf_1gram, tfidf_1gram = TFIDF(train_df_corpus, 1)\n",
        "train_df_tfidf_2gram, tfidf_2gram = TFIDF(train_df_corpus, 2)\n",
        "train_df_tfidf_3gram, tfidf_3gram = TFIDF(train_df_corpus, 3)\n",
        "\n",
        "print(len(train_df_corpus))\n",
        "print(train_df_tfidf_1gram.shape)\n",
        "print(train_df_tfidf_1gram.shape)\n",
        "print(train_df_tfidf_1gram.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "mZKYNsfeSR-P"
      },
      "cell_type": "code",
      "source": [
        "del train_df_tfidf_1gram, train_df_tfidf_2gram, train_df_tfidf_3gram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vS4PX-QxSR-P"
      },
      "cell_type": "markdown",
      "source": [
        "## Word Embedding:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "Uj1yKOa8SR-Q"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "import gensim\n",
        "print(\"gensim version:\", gensim.__version__)\n",
        "\n",
        "word2vec_path = \"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\"\n",
        "\n",
        "# we only load 200k most common words from Google News corpus\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=200000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lGONAGA9SR-Q"
      },
      "cell_type": "markdown",
      "source": [
        "Compare the similarity between \"cat\" vs. \"kitten\" and \"cat\" vs. \"cats\""
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9A4lvy1xSR-Q"
      },
      "cell_type": "code",
      "source": [
        "print(word2vec_model.similarity('cat', 'kitten'))\n",
        "print(word2vec_model.similarity('cat', 'cats'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "fZZgNPdUSR-Q"
      },
      "cell_type": "code",
      "source": [
        "def get_average_vec(tokens_list, vector, generate_missing=False, k=300):\n",
        "    \"\"\"\n",
        "        Calculate average embedding value of sentence from each word vector\n",
        "    \"\"\"\n",
        "\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_embeddings(vectors, text, generate_missing=False, k=300):\n",
        "    \"\"\"\n",
        "        create the sentence embedding\n",
        "    \"\"\"\n",
        "    embeddings = text.apply(lambda x: get_average_vec(x, vectors, generate_missing=generate_missing, k=k))\n",
        "    return list(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "dqKqKvOvSR-Q"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "embeddings_word2vec = get_embeddings(word2vec_model, train_df[\"lemmatize_text\"], k=300)\n",
        "\n",
        "print(\"Embedding matrix size\", len(embeddings_word2vec), len(embeddings_word2vec[0]))\n",
        "print(\"The sentence: \\\"%s\\\" got embedding values: \" % train_df[\"lemmatize_text\"][0])\n",
        "print(embeddings_word2vec[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "W27iXZ7-SR-Q"
      },
      "cell_type": "code",
      "source": [
        "del embeddings_word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gc758VtYSR-Q"
      },
      "cell_type": "markdown",
      "source": [
        "#### Global Vectors for Word Representation (GloVe):"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "SX7j00RFSR-Q"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove_input_file = \"../input/glove6b/glove.6B.300d.txt\"\n",
        "word2vec_output_file = \"glove.6B.100d.txt.word2vec\"\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "\n",
        "# we only load 200k most common words from Google New corpus\n",
        "glove_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False, limit=200000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1J1rca0KSR-R"
      },
      "cell_type": "code",
      "source": [
        "print(glove_model.similarity('cat', 'kitten'))\n",
        "print(glove_model.similarity('cat', 'cats'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3F_mTcicSR-R"
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "embeddings_glove = get_embeddings(glove_model, train_df[\"lemmatize_text\"], k=300)\n",
        "\n",
        "print(\"Embedding matrix size\", len(embeddings_glove), len(embeddings_glove[0]))\n",
        "print(\"The sentence: \\\"%s\\\" got embedding values: \" % train_df[\"lemmatize_text\"][0])\n",
        "print(embeddings_glove[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}